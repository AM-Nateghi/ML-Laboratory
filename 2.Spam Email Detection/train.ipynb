{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìß Spam Email Detection Project\n",
    "\n",
    "This notebook implements a spam email detection system using TF-IDF vectorization and multiple machine learning models.\n",
    "\n",
    "## Models Used:\n",
    "- RandomForest\n",
    "- GradientBoosting \n",
    "- NaiveBayes\n",
    "\n",
    "## Evaluation Metric:\n",
    "- Jaccard Score\n",
    "- Accuracy Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sklearn libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    jaccard_score, \n",
    "    classification_report, \n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score\n",
    ")\n",
    "\n",
    "# Text preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    nltk.download('punkt_tab')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('email_classification_dataset.csv')\n",
    "\n",
    "print(\"üìä Dataset Information:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(\"\\nüìà Label Distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "print(f\"\\nüìß Sample email length: {len(df.iloc[0]['email'])} characters\")\n",
    "\n",
    "# Display first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize label distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(data=df, x='label', palette='viridis')\n",
    "plt.title('Distribution of Email Labels (Ham vs Spam)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nüîç Missing Values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Comprehensive text preprocessing function\n",
    "    \n",
    "    Steps:\n",
    "    1. Convert to lowercase\n",
    "    2. Remove special characters and numbers\n",
    "    3. Remove extra whitespaces\n",
    "    4. Tokenize\n",
    "    5. Remove stopwords\n",
    "    6. Apply stemming\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove emails, URLs, and special patterns\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)  # Remove emails\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters and numbers\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Test preprocessing function\n",
    "sample_text = df.iloc[0]['email']\n",
    "print(\"üìß Original text (first 200 chars):\")\n",
    "print(sample_text[:200])\n",
    "print(\"\\nüîß Preprocessed text:\")\n",
    "preprocessed = preprocess_text(sample_text)\n",
    "print(preprocessed[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to all emails\n",
    "print(\"üîÑ Preprocessing all emails...\")\n",
    "df['cleaned_email'] = df['email'].apply(preprocess_text)\n",
    "\n",
    "# Check the results\n",
    "print(\"‚úÖ Preprocessing completed!\")\n",
    "print(f\"\\nüìä Average email length before preprocessing: {df['email'].str.len().mean():.2f} characters\")\n",
    "print(f\"üìä Average email length after preprocessing: {df['cleaned_email'].str.len().mean():.2f} characters\")\n",
    "\n",
    "# Remove empty emails after preprocessing\n",
    "df = df[df['cleaned_email'].str.len() > 0]\n",
    "print(f\"\\nüìà Final dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,  # Keep top 5000 features\n",
    "    ngram_range=(1, 2),  # Use unigrams and bigrams\n",
    "    min_df=2,  # Ignore terms that appear in less than 2 documents\n",
    "    max_df=0.95,  # Ignore terms that appear in more than 95% of documents\n",
    "    sublinear_tf=True  # Apply log scaling\n",
    ")\n",
    "\n",
    "# Fit and transform the cleaned emails\n",
    "print(\"üîÑ Applying TF-IDF vectorization...\")\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['cleaned_email'])\n",
    "y = df['label']\n",
    "\n",
    "print(f\"‚úÖ TF-IDF vectorization completed!\")\n",
    "print(f\"üìä Feature matrix shape: {X_tfidf.shape}\")\n",
    "print(f\"üìä Number of features: {len(tfidf_vectorizer.get_feature_names_out())}\")\n",
    "print(f\"üìä Sparsity: {(1 - X_tfidf.nnz / X_tfidf.size) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_tfidf, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y  # Maintain class distribution\n",
    ")\n",
    "\n",
    "print(\"üìä Data Split Information:\")\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"\\nüìà Training set label distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nüìà Test set label distribution:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=20,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'GradientBoosting': GradientBoostingClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'NaiveBayes': MultinomialNB(alpha=1.0)\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "print(\"üöÄ Training models...\\n\")\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    jaccard = jaccard_score(y_test, y_pred, pos_label='spam')  # Using 'spam' as positive class\n",
    "    f1 = f1_score(y_test, y_pred, pos_label='spam')\n",
    "    precision = precision_score(y_test, y_pred, pos_label='spam')\n",
    "    recall = recall_score(y_test, y_pred, pos_label='spam')\n",
    "    \n",
    "    # Store results\n",
    "    results[model_name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'jaccard': jaccard,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'model': model\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ {model_name} Results:\")\n",
    "    print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"   Jaccard Score: {jaccard:.4f}\")\n",
    "    print(f\"   F1 Score: {f1:.4f}\")\n",
    "    print(f\"   Precision: {precision:.4f}\")\n",
    "    print(f\"   Recall: {recall:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"üéâ All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Comparison and Best Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Accuracy': [results[model]['accuracy'] for model in results],\n",
    "    'Jaccard Score': [results[model]['jaccard'] for model in results],\n",
    "    'F1 Score': [results[model]['f1'] for model in results],\n",
    "    'Precision': [results[model]['precision'] for model in results],\n",
    "    'Recall': [results[model]['recall'] for model in results]\n",
    "})\n",
    "\n",
    "# Sort by Jaccard Score (as requested)\n",
    "comparison_df = comparison_df.sort_values('Jaccard Score', ascending=False)\n",
    "\n",
    "print(\"üìä Model Comparison Results (Sorted by Jaccard Score):\")\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# Find best model\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_jaccard_score = comparison_df.iloc[0]['Jaccard Score']\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
    "print(f\"üéØ Best Jaccard Score: {best_jaccard_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[0, 0].bar(comparison_df['Model'], comparison_df['Accuracy'], color='skyblue')\n",
    "axes[0, 0].set_title('Accuracy Scores')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(comparison_df['Accuracy']):\n",
    "    axes[0, 0].text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
    "\n",
    "# Jaccard Score comparison\n",
    "axes[0, 1].bar(comparison_df['Model'], comparison_df['Jaccard Score'], color='lightgreen')\n",
    "axes[0, 1].set_title('Jaccard Scores')\n",
    "axes[0, 1].set_ylabel('Jaccard Score')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(comparison_df['Jaccard Score']):\n",
    "    axes[0, 1].text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
    "\n",
    "# F1 Score comparison\n",
    "axes[1, 0].bar(comparison_df['Model'], comparison_df['F1 Score'], color='orange')\n",
    "axes[1, 0].set_title('F1 Scores')\n",
    "axes[1, 0].set_ylabel('F1 Score')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(comparison_df['F1 Score']):\n",
    "    axes[1, 0].text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
    "\n",
    "# Combined metrics\n",
    "x = range(len(comparison_df))\n",
    "width = 0.25\n",
    "axes[1, 1].bar([i - width for i in x], comparison_df['Precision'], width, label='Precision', alpha=0.8)\n",
    "axes[1, 1].bar(x, comparison_df['Recall'], width, label='Recall', alpha=0.8)\n",
    "axes[1, 1].bar([i + width for i in x], comparison_df['F1 Score'], width, label='F1 Score', alpha=0.8)\n",
    "axes[1, 1].set_title('Precision, Recall, and F1 Score')\n",
    "axes[1, 1].set_ylabel('Score')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(comparison_df['Model'], rotation=45)\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Detailed Analysis of Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model\n",
    "best_model = results[best_model_name]['model']\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "# Detailed classification report\n",
    "print(f\"üìã Detailed Classification Report for {best_model_name}:\")\n",
    "print(classification_report(y_test, y_pred_best))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Ham', 'Spam'], \n",
    "            yticklabels=['Ham', 'Spam'])\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Importance Analysis (for tree-based models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis (only for tree-based models)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    # Get feature names\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Get feature importances\n",
    "    importances = best_model.feature_importances_\n",
    "    \n",
    "    # Create DataFrame for easy sorting\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Display top 20 most important features\n",
    "    print(f\"üéØ Top 20 Most Important Features for {best_model_name}:\")\n",
    "    print(feature_importance_df.head(20))\n",
    "    \n",
    "    # Plot top 15 features\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    top_features = feature_importance_df.head(15)\n",
    "    sns.barplot(data=top_features, y='feature', x='importance', palette='viridis')\n",
    "    plt.title(f'Top 15 Feature Importances - {best_model_name}')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Feature importance not available for {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_email(email_text, model=best_model, vectorizer=tfidf_vectorizer):\n",
    "    \"\"\"\n",
    "    Predict whether an email is spam or ham\n",
    "    \n",
    "    Args:\n",
    "        email_text (str): The email text to classify\n",
    "        model: Trained model to use for prediction\n",
    "        vectorizer: Fitted TF-IDF vectorizer\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (prediction, probability)\n",
    "    \"\"\"\n",
    "    # Preprocess the email\n",
    "    cleaned_email = preprocess_text(email_text)\n",
    "    \n",
    "    # Vectorize\n",
    "    email_tfidf = vectorizer.transform([cleaned_email])\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(email_tfidf)[0]\n",
    "    probability = model.predict_proba(email_tfidf)[0]\n",
    "    \n",
    "    return prediction, probability\n",
    "\n",
    "# Test with sample emails\n",
    "test_emails = [\n",
    "    \"Congratulations! You've won $1000000! Click here to claim your prize now!\",\n",
    "    \"Hi, this is a reminder about your appointment tomorrow at 2 PM. Please confirm.\",\n",
    "    \"URGENT: Your account will be suspended unless you verify your details immediately!\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing the best model with sample emails:\\n\")\n",
    "for i, email in enumerate(test_emails, 1):\n",
    "    prediction, probability = predict_email(email)\n",
    "    spam_prob = probability[1] if prediction == 'spam' else probability[0]\n",
    "    \n",
    "    print(f\"Email {i}: {email[:50]}...\")\n",
    "    print(f\"Prediction: {prediction.upper()}\")\n",
    "    print(f\"Confidence: {spam_prob:.2%}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìã SPAM EMAIL DETECTION PROJECT SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üìä Dataset Size: {df.shape[0]} emails\")\n",
    "print(f\"üîß Preprocessing: Text cleaning, tokenization, stemming\")\n",
    "print(f\"üéØ Vectorization: TF-IDF with {X_tfidf.shape[1]} features\")\n",
    "print(f\"ü§ñ Models Tested: {', '.join(models.keys())}\")\n",
    "print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
    "print(f\"üéØ Jaccard Score: {best_jaccard_score:.4f}\")\n",
    "print(f\"üìà Accuracy: {results[best_model_name]['accuracy']:.4f}\")\n",
    "print(f\"üé™ F1 Score: {results[best_model_name]['f1']:.4f}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"‚úÖ Project completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}